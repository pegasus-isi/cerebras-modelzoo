{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e243d342",
   "metadata": {},
   "source": [
    "# Cerebras ModelZoo PyTorch Training Workflow\n",
    "\n",
    "\n",
    "Neocortex, a system that captures groundbreaking new hardware technologies, is designed to accelerate Artificial Intelligence (AI) research in pursuit of science, discovery, and societal good. Neocortex is a highly innovative resource that will accelerate AI- powered scientific discovery by vastly shortening the time required for deep learning training, foster greater integration of artificial deep learning with scientific workflows, and provide revolutionary new hardware for the development of more efficient algorithms for artificial intelligence and graph analytics.\n",
    "\n",
    "In this notebook, we will create a Pegasus workflow to validate, compile and train the reference PyTorch model in the Cerebras modelzoo repository, to run on Neocortex.\n",
    "\n",
    "## Pegasus Concepts and Training\n",
    "\n",
    "It is highly recommended that you do Pegasus training on concepts of defining a workflow in Pegasus and how to configure the various catalogs via [ACCESS Pegasus](https://pegasus.access-ci.org). Click on the Try Pegasus Button on the page there.\n",
    "\n",
    "\n",
    "## Container\n",
    "\n",
    "All the jobs are run via a Cerebras provided singularity container that is available on the shared filesystem on Neocortex\n",
    "\n",
    "\n",
    "## Setting up the input data for the workflow\n",
    "\n",
    "The workflow also requires additional inputs to be placed in the input directory namely the **params.yaml** file for the training. The training datasets are tracked as part of the workflow and downloaded at runtime from the http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "These are the inputs on which you do the training\n",
    "\n",
    "modelzoo/modelzoo/fc_mnist/pytorch/data/mnist/train/MNIST/raw/train-images-idx3-ubyte.gz\n",
    "modelzoo/modelzoo/fc_mnist/pytorch/data/mnist/train/MNIST/raw/train-labels-idx1-ubyte.gz\n",
    "modelzoo/modelzoo/fc_mnist/pytorch/data/mnist/train/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
    "modelzoo/modelzoo/fc_mnist/pytorch/data/mnist/train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "The compute jobs will run on the cerebras cs-2 nodes. Pegasus adds other jobs that are run on the login node and retrieve the input data required for the workflow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./executables/prepare_inputs.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5bc70a",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The Pegasus workflow starts with a tar file containing the Git checkout of the modelzoo repo, and iterates on it through the 3 stages. The `prepare_inputs.sh` script in the previous script does the checkout of the modelzoo github repository and tars it.\n",
    "\n",
    "We then track it as an input for the workflow.\n",
    "\n",
    "![Pegasus Cerebras Pytorch Workflow](./images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca941c9",
   "metadata": {},
   "source": [
    "### Set the Jupyter Environment\n",
    "\n",
    "We set some environment variables and set PYTHONPATH for Pegasus libraries to be imported successfully. This is temporary until the Jupyter Notebook setup is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=/ocean/projects/cis240026p/vahi/software/install/pegasus/default/lib64/pegasus/python:/jet/home/vahi/pegasus-env/lib/python3.6/site-packages:$PYTHONPATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/ocean/projects/cis240026p/vahi/software/install/pegasus/default/lib64/pegasus/python\")\n",
    "sys.path.append(\"/jet/home/vahi/pegasus-env/lib/python3.6/site-packages\")\n",
    "print(os.environ.get('PYTHONPATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e439aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample Pegasus workflow for training a model on the Cerebras resource\n",
    "at Neocortex.\n",
    "\n",
    "This workflow validates, compiles and trains the model as part\n",
    "of a single worklfow setup to run on Neocortex.\n",
    "\n",
    "https://portal.neocortex.psc.edu/docs/running-jobs.html\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = str(Path(\".\").resolve())\n",
    "\n",
    "# need to know where Pegasus is installed for notifications\n",
    "PEGASUS_HOME = shutil.which(\"pegasus-version\")\n",
    "PEGASUS_HOME = os.path.dirname(os.path.dirname(PEGASUS_HOME))\n",
    "\n",
    "# the PROJECT you are part of\n",
    "PROJECT=\"cis240026p\"\n",
    "\n",
    "class CerebrasPyTorchWorkflow():\n",
    "    wf = None\n",
    "    sc = None\n",
    "    tc = None\n",
    "    rc = None\n",
    "    props = None\n",
    "    wf_name = \"cerebras-model-zoo-pt\"\n",
    "    project = None\n",
    "    # Log\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # --- Init ---------------------------------------------------------------------\n",
    "    def __init__(self, dagfile=\"workflow.yml\", project=None):\n",
    "        self.dagfile = dagfile\n",
    "        self.project = project\n",
    "\n",
    "    # --- Write files in directory -------------------------------------------------\n",
    "    def write(self):\n",
    "        if not self.sc is None:\n",
    "            self.sc.write()\n",
    "        self.props.write()\n",
    "        self.rc.write()\n",
    "        self.tc.write()\n",
    "\n",
    "        try:\n",
    "            self.wf.write()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Plan and Submit the workflow ----------------------------------------------\n",
    "    def plan_submit(self):\n",
    "        try:\n",
    "            self.wf.plan(\n",
    "                conf=\"pegasus.properties\",\n",
    "                sites=[\"neocortex\"],\n",
    "                output_site=\"local\",\n",
    "                dir=\"submit\",\n",
    "                cleanup=\"none\",\n",
    "                force=True,\n",
    "                verbose=5,\n",
    "                submit=True,\n",
    "            )\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Get status of the workflow -----------------------------------------------\n",
    "    def status(self):\n",
    "        try:\n",
    "            self.wf.status(long=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Wait for the workflow to finish -----------------------------------------------\n",
    "    def wait(self):\n",
    "        try:\n",
    "            self.wf.wait()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Get statistics of the workflow -----------------------------------------------\n",
    "    def statistics(self):\n",
    "        try:\n",
    "            self.wf.statistics()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Configuration (Pegasus Properties) ---------------------------------------\n",
    "    def create_pegasus_properties(self):\n",
    "        self.props = Properties()\n",
    "        self.props[\"pegasus.integrity.checking\"] = \"none\"\n",
    "        self.props[\n",
    "            \"pegasus.catalog.workflow.amqp.url\"\n",
    "        ] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "        self.props[\"pegasus.data.configuration\"] = \"nonsharedfs\"\n",
    "        self.props[\"pegasus.mode\"] = \"development\"\n",
    "        # data transfers for the jobs should happen\n",
    "        # on the HOSTOS not inside the container\n",
    "        self.props[\"pegasus.transfer.container.onhost\"] = True\n",
    "        # we dont want any pegasus worker package\n",
    "        # to be installed inside the container\n",
    "        self.props[\"pegasus.transfer.worker.package\"] = True\n",
    "        self.props[\"pegasus.transfer.worker.package.autodownload\"] = False\n",
    "        # enable symlinking\n",
    "        # props[\"pegasus.transfer.links\"] = True\n",
    "        self.props.write()\n",
    "        return\n",
    "\n",
    "    # --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.sc = SiteCatalog()\n",
    "        # add a local site with an optional job env file to use for compute jobs\n",
    "        shared_scratch_dir = \"/{}/workflows/LOCAL/scratch\".format(\"${PROJECT}\")\n",
    "        local_storage_dir = \"{}/outputs\".format(BASE_DIR)\n",
    "        local = Site(\"local\").add_directories(\n",
    "            Directory(Directory.SHARED_SCRATCH, shared_scratch_dir).add_file_servers(\n",
    "                FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)\n",
    "            ),\n",
    "            Directory(Directory.LOCAL_STORAGE, local_storage_dir).add_file_servers(\n",
    "                FileServer(\"file://\" + local_storage_dir, Operation.ALL)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sc.add_sites(local)\n",
    "\n",
    "        shared_scratch_dir = \"/{}/workflows/NEOCORTEX/scratch\".format(\"${PROJECT}\")\n",
    "        local_scratch_dir = \"/local4/{}\".format(\"${SALLOC_ACCOUNT}\")\n",
    "        neocortex = Site(\"neocortex\").add_directories(\n",
    "            Directory(\n",
    "                Directory.SHARED_SCRATCH, shared_scratch_dir, shared_file_system=True\n",
    "            ).add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "            Directory(Directory.LOCAL_SCRATCH, local_scratch_dir).add_file_servers(\n",
    "                FileServer(\"file://\" + local_scratch_dir, Operation.ALL)\n",
    "            ),\n",
    "        )\n",
    "        neocortex.add_condor_profile(grid_resource=\"batch slurm\")\n",
    "        #    neocortex.add_env(\"PEGASUS_HOME\", \"/ocean/projects/cis240026p/vahi/software/install/pegasus/default\")\n",
    "        neocortex.add_pegasus_profile(\n",
    "            style=\"glite\",\n",
    "            queue=\"sdf\",\n",
    "            auxillary_local=True,\n",
    "            runtime=1800,\n",
    "            project=self.project,\n",
    "        )\n",
    "        self.sc.add_sites(neocortex)\n",
    "\n",
    "    # --- Transformation Catalog (Executables and Containers) ----------------------\n",
    "    def create_transformation_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.tc = TransformationCatalog()\n",
    "        container = Container(\n",
    "            \"cerebras\",\n",
    "            Container.SINGULARITY,\n",
    "            \"file:///ocean/neocortex/cerebras/cbcore_latest.sif\",\n",
    "            image_site=\"neocortex\",\n",
    "            # mounts=['/${PROJECT}/workflows/NEOCORTEX/scratch:/${PROJECT}/workflows/NEOCORTEX/scratch:rw'],\n",
    "        )\n",
    "        self.tc.add_containers(container)\n",
    "\n",
    "        validate = Transformation(\n",
    "            \"validate\",\n",
    "            site=\"local\",\n",
    "            pfn=BASE_DIR + \"/executables/validate.sh\",\n",
    "            is_stageable=True,\n",
    "            container=container,\n",
    "        )\n",
    "        validate.add_profiles(Namespace.PEGASUS, key=\"cores\", value=\"1\")\n",
    "        validate.add_profiles(Namespace.PEGASUS, key=\"runtime\", value=\"900\")\n",
    "        validate.add_profiles(\n",
    "            Namespace.PEGASUS,\n",
    "            key=\"glite.arguments\",\n",
    "            value=\"--cpus-per-task=14 --gres=cs:cerebras:1 --qos=low\",\n",
    "        )\n",
    "        self.tc.add_transformations(validate)\n",
    "\n",
    "        compile = Transformation(\n",
    "            \"compile\",\n",
    "            site=\"local\",\n",
    "            pfn=BASE_DIR + \"/executables/compile.sh\",\n",
    "            is_stageable=True,\n",
    "            container=container,\n",
    "        )\n",
    "        compile.add_profiles(Namespace.PEGASUS, key=\"cores\", value=\"1\")\n",
    "        compile.add_profiles(Namespace.PEGASUS, key=\"runtime\", value=\"900\")\n",
    "        compile.add_profiles(\n",
    "            Namespace.PEGASUS,\n",
    "            key=\"glite.arguments\",\n",
    "            value=\"--cpus-per-task=14 --gres=cs:cerebras:1 --qos=low\",\n",
    "        )\n",
    "        self.tc.add_transformations(compile)\n",
    "\n",
    "        train = Transformation(\n",
    "            \"train\", site=\"local\", pfn=BASE_DIR + \"/executables/train.sh\", is_stageable=True, container=container\n",
    "        )\n",
    "        train.add_profiles(Namespace.PEGASUS, key=\"cores\", value=\"1\")\n",
    "        train.add_profiles(Namespace.PEGASUS, key=\"runtime\", value=\"3600\")\n",
    "        train.add_profiles(Namespace.PEGASUS, key=\"container.launcher\", value=\"srun\")\n",
    "        train.add_profiles(Namespace.PEGASUS, key=\"container.launcher.arguments\", value=\"--kill-on-bad-exit\")\n",
    "        train.add_profiles(\n",
    "            Namespace.PEGASUS,\n",
    "            key=\"glite.arguments\",\n",
    "            value=\"--cpus-per-task=14 --gres=cs:cerebras:1 --qos=low\"\n",
    "        )\n",
    "        self.tc.add_transformations(train)\n",
    "\n",
    "    # --- Replica Catalog ----------------------------------------------------------\n",
    "    def create_replica_catalog(self):\n",
    "        self.rc = ReplicaCatalog()\n",
    "        # most of the replicas are added when creating the workflow\n",
    "\n",
    "    # --- Create Workflow ----------------------------------------------------------\n",
    "    def create_workflow(self):\n",
    "        self.wf = Workflow(self.wf_name)\n",
    "\n",
    "        # --- Workflow -----------------------------------------------------\n",
    "        # the main input for the workflow is config file and the modelzoo checkout\n",
    "        modelzoo_config_params = File(\n",
    "            \"modelzoo/modelzoo/fc_mnist/pytorch/configs/params.yaml\"\n",
    "        )\n",
    "        modelzoo_raw = File(\"modelzoo-raw.tgz\")\n",
    "        self.rc.add_replica(\n",
    "            \"local\", modelzoo_config_params.lfn, \"{}/input/params.yaml\".format(BASE_DIR)\n",
    "        )\n",
    "        self.rc.add_replica(\n",
    "            \"local\", modelzoo_raw.lfn, \"{}/input/modelzoo-raw.tgz\".format(BASE_DIR)\n",
    "        )\n",
    "\n",
    "        # some output of modelzoo checkout at each stage\n",
    "        modelzoo_validated = File(\"modelzoo-validated.tgz\")\n",
    "        modelzoo_compiled = File(\"modelzoo-compiled.tgz\")\n",
    "        modelzoo_trained = File(\"modelzoo-trained.tgz\")\n",
    "        modelzoo_trained_checkpoints = File(\"model-checkpoints.tgz\")\n",
    "\n",
    "        # some logs that we always stageout\n",
    "        cerebras_logs = [\"fabric.json\", \"run_summary.json\", \"params.yaml\"]\n",
    "\n",
    "        # validate job\n",
    "        validate_job = Job(\"validate\", node_label=\"validate_model\")\n",
    "        validate_job.add_args(\n",
    "            \"--mode train --validate_only --params configs/params.yaml  --model_dir model\"\n",
    "        )\n",
    "        validate_job.add_inputs(modelzoo_raw)\n",
    "        validate_job.add_inputs(modelzoo_config_params)\n",
    "        validate_job.add_outputs(modelzoo_validated, stage_out=True)\n",
    "        # add files against which we will train as inputs\n",
    "        # instead of letting the code download automatically\n",
    "        prefix = \"modelzoo/modelzoo/fc_mnist/pytorch/data/mnist/train/MNIST/raw\"\n",
    "        for file in [\n",
    "            \"train-images-idx3-ubyte.gz\",\n",
    "            \"train-labels-idx1-ubyte.gz\",\n",
    "            \"t10k-images-idx3-ubyte.gz\",\n",
    "            \"t10k-labels-idx1-ubyte.gz\",\n",
    "        ]:\n",
    "            train_file = File(\"{}/{}\".format(prefix, file))\n",
    "            self.rc.add_replica(\n",
    "                \"nonlocal\",\n",
    "                train_file.lfn,\n",
    "                \"http://yann.lecun.com/exdb/mnist/{}\".format(file),\n",
    "            )\n",
    "            self.rc.add_replica(\n",
    "                \"nonlocal\",\n",
    "                train_file.lfn,\n",
    "                \"https://ossci-datasets.s3.amazonaws.com/mnist/{}\".format(file),\n",
    "            )\n",
    "            validate_job.add_inputs(train_file)\n",
    "\n",
    "        # track some cerebras log files as outputs\n",
    "        for file in cerebras_logs:\n",
    "            # scripts do rename of the files after job completes\n",
    "            validate_job.add_outputs(File(\"{}_{}\".format(\"validate\", file)), stage_out=True)\n",
    "\n",
    "        self.wf.add_jobs(validate_job)\n",
    "\n",
    "        # compile job\n",
    "        compile_job = Job(\"compile\", node_label=\"compile_model\")\n",
    "        compile_job.add_args(\n",
    "            \"--mode train --compile_only --params configs/params.yaml --model_dir model\"\n",
    "        )\n",
    "        compile_job.add_inputs(modelzoo_validated)\n",
    "        compile_job.add_outputs(modelzoo_compiled, stage_out=True)\n",
    "\n",
    "        # track some cerebras log files as outputs\n",
    "        for file in cerebras_logs:\n",
    "            # scripts do rename of the files after job completes\n",
    "            compile_job.add_outputs(File(\"{}_{}\".format(\"compile\", file)), stage_out=True)\n",
    "\n",
    "        self.wf.add_jobs(compile_job)\n",
    "\n",
    "        # training job\n",
    "        now = datetime.datetime.now().strftime(\"%s\")\n",
    "        training_job = Job(\"train\", node_label=\"train_model\")\n",
    "        training_job.add_args(\n",
    "            \"--mode train --params configs/params.yaml --model_dir model --cs_ip $CS_IP_ADDR\"\n",
    "        )\n",
    "        training_job.add_inputs(modelzoo_compiled)\n",
    "        training_job.add_outputs(modelzoo_trained, stage_out=True)\n",
    "        # training_job.add_outputs(modelzoo_trained_checkpoints, stage_out=True)\n",
    "        training_job.set_stdout(\"train-{}.out\".format(now))\n",
    "        training_job.set_stderr(\"train-{}.err\".format(now))\n",
    "\n",
    "        # track some cerebras log files as outputs\n",
    "        for file in cerebras_logs:\n",
    "            # scripts do rename of the files after job completes\n",
    "            if file == \"fabric.json\":\n",
    "                # we dont copy fabric.json\n",
    "                continue\n",
    "            training_job.add_outputs(File(\"{}_{}\".format(\"train\", file)), stage_out=True)\n",
    "\n",
    "        training_job.add_outputs(File(\"train_performance.json\"), stage_out=True)\n",
    "        self.wf.add_jobs(training_job)\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        self.log.info(\"Creating workflow properties...\")\n",
    "        self.create_pegasus_properties()\n",
    "\n",
    "        self.log.info(\"Creating execution sites...\")\n",
    "        self.create_sites_catalog()\n",
    "\n",
    "        self.log.info(\"Creating transformation catalog...\")\n",
    "        self.create_transformation_catalog()\n",
    "\n",
    "        self.log.info(\"Creating replica catalog...\")\n",
    "        self.create_replica_catalog()\n",
    "\n",
    "        self.log.info(\"Creating workflow ...\")\n",
    "        self.create_workflow()\n",
    "\n",
    "        self.write()\n",
    "        self.log.info(\"Workflow has been created. Will be planned and submitted ...\")\n",
    "\n",
    "        #self.plan_submit()\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "workflow = CerebrasPyTorchWorkflow(project=PROJECT)\n",
    "try:\n",
    "    workflow()\n",
    "except PegasusClientError as e:\n",
    "    workflow.log.debug(\"\", exc_info=True)\n",
    "    print(e.output)\n",
    "    sys.exit(1)\n",
    "except Exception:\n",
    "    workflow.log.debug(\"\", exc_info=True)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3adb46",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site **condorpool** i.e the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.plan_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76c63a",
   "metadata": {},
   "source": [
    "After the workflow has been successfully planned and submitted, you can use the Python `Workflow` object in order to monitor the status of the workflow. It shows in detail the counts of jobs of each status and also the whether the job is idle or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3380d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5a1d4",
   "metadata": {},
   "source": [
    "## Wait for the workflow to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b26aa",
   "metadata": {},
   "source": [
    "## Inspect the outputs\n",
    "\n",
    "The outputs of the workflow run can be found in the `./outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
